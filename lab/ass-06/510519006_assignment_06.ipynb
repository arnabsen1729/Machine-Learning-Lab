{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_XXsTdDki7X"
      },
      "source": [
        "# Assignment-6\n",
        "- Name - Arnab Sen\n",
        "- En No. - 510519006"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdCZhKE4li_k"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnDTIzrvlj-i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import \\\n",
        "    Conv2D, Dense, Input, MaxPool2D, Flatten, \\\n",
        "    Dropout, AvgPool2D, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.math import confusion_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "DATASET_DIRECTORY_PATH = '/content/drive/MyDrive/ML_DRIVE/Assign_6/dataset/'\n",
        "seed = random.randint(0,100)\n",
        "LRELU_APLHA = 0.01\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" #ignore CUDA messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-u7DmUBlxiC"
      },
      "source": [
        "### Task-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MTzUWGtki7Z",
        "outputId": "93917d65-f9fc-4dc4-bcc6-fb956a72ffba"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory= DATASET_DIRECTORY_PATH,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\", #output of model will be softmax categorical\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=32,\n",
        "    image_size=(80,80),\n",
        "    shuffle=True,\n",
        "    seed=seed, #same seed for both dataset so that no overlap happens\n",
        "    validation_split=0.1, #90:10 split\n",
        "    subset='training',\n",
        "    crop_to_aspect_ratio=False,\n",
        ")\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory= DATASET_DIRECTORY_PATH,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\", #output of model will be softmax categorical\n",
        "    color_mode=\"grayscale\",\n",
        "    batch_size=32,\n",
        "    image_size=(80,80),\n",
        "    shuffle=True,\n",
        "    seed=seed,\n",
        "    validation_split=0.1, #90:10 split\n",
        "    subset='validation',\n",
        "    crop_to_aspect_ratio=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gnIEhbTki7b"
      },
      "source": [
        "## Task 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7v3ClJmki7b"
      },
      "outputs": [],
      "source": [
        "input_shape = (80, 80, 1)\n",
        "num_class = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATQRoVMYki7b"
      },
      "outputs": [],
      "source": [
        "def _plot_history(\n",
        "    history: 'tf.keras.callbacks.History',\n",
        "    conv_kernels: 'list[tuple[int, int]]',\n",
        "    conv_filters: 'list[int]',\n",
        "    activation: 'str',\n",
        "    pool: 'str',\n",
        "    num_fc_layers: 'int'\n",
        "):\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.title(f'Loss vs epoch')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend()\n",
        "    plt.title(f'Accuracy vs epoch')\n",
        "\n",
        "    plt.suptitle(\n",
        "        f'filters {conv_filters}; kernels {conv_kernels}; {pool}_pool; {activation}; dense layers after flatten= {num_fc_layers}')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def _plot_confusion_matrix(y_val: 'list[int]',\n",
        "                           y_pred: 'list[int]'):\n",
        "    matrix = confusion_matrix(y_val, y_pred)\n",
        "    fig = plt.figure(figsize=(12, 5))\n",
        "    sns.heatmap(\n",
        "        matrix,\n",
        "        xticklabels=range(1, num_class + 1),\n",
        "        yticklabels=range(1, num_class + 1),\n",
        "        linewidth=0.5,\n",
        "        cmap='coolwarm',\n",
        "        annot=True,\n",
        "        cbar=True)\n",
        "    plt.title('Confusion Matrix for the above model')\n",
        "    plt.ylabel('Actual Value')\n",
        "    plt.xlabel('Predicted Value')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    conv_kernels: 'list[tuple[int, int]]',\n",
        "    conv_filters: 'list[int]',\n",
        "    activation: 'str',\n",
        "    pool: 'str',\n",
        "    drop_rate: 'float',\n",
        "    num_fc_layers: 'int',\n",
        "    train_dataset: \"tf.data.Dataset\",\n",
        "    val_dataset: \"tf.data.Dataset\",\n",
        "    add_batch_norm=False,\n",
        "    fc_layer_size=64,\n",
        "    num_epochs=100,\n",
        "    extra_conv_layers=0,\n",
        "    give_model=False\n",
        "):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "\n",
        "    for filtr, kernel in zip(conv_filters, conv_kernels):\n",
        "        if activation == 'lrelu':\n",
        "            model.add(Conv2D(\n",
        "                filters=filtr,\n",
        "                kernel_size=kernel,\n",
        "                activation=LeakyReLU(alpha=LRELU_APLHA)\n",
        "            ))\n",
        "        else:\n",
        "            model.add(Conv2D(\n",
        "                filters=filtr,\n",
        "                kernel_size=kernel,\n",
        "                activation=activation\n",
        "            ))\n",
        "\n",
        "        if pool.lower() == 'max':\n",
        "            model.add(MaxPool2D())\n",
        "        elif pool.lower() == 'avg':\n",
        "            model.add(AvgPool2D())\n",
        "        else:\n",
        "            raise Exception('argument pool is undefined')\n",
        "\n",
        "        if add_batch_norm:\n",
        "            model.add(BatchNormalization())\n",
        "\n",
        "        if drop_rate > 0:\n",
        "            model.add(Dropout(rate=drop_rate))\n",
        "\n",
        "    for i in range(0, extra_conv_layers):\n",
        "        conv_filters.append(conv_filters[-1]*2)\n",
        "        conv_kernels.append(conv_kernels[-1])\n",
        "\n",
        "        if activation == 'lrelu':\n",
        "            model.add(Conv2D(\n",
        "                filters=conv_filters[-1],\n",
        "                kernel_size=conv_kernels[-1],\n",
        "                activation=LeakyReLU(alpha=LRELU_APLHA),\n",
        "                padding='same'\n",
        "            ))\n",
        "        else:\n",
        "            model.add(Conv2D(\n",
        "                filters=conv_filters[-1],\n",
        "                kernel_size=conv_kernels[-1],\n",
        "                activation=activation,\n",
        "                padding='same'\n",
        "            ))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    for _ in range(num_fc_layers):\n",
        "        if activation == 'lrelu':\n",
        "            model.add(Dense(\n",
        "                units=fc_layer_size,\n",
        "                activation=LeakyReLU(alpha=LRELU_APLHA)\n",
        "            ))\n",
        "        else:\n",
        "            model.add(Dense(\n",
        "                units=fc_layer_size,\n",
        "                activation=activation\n",
        "            ))\n",
        "\n",
        "    model.add(Dense(units=num_class, activation='softmax'))\n",
        "\n",
        "\n",
        "    model.compile(loss=CategoricalCrossentropy(),\n",
        "                  metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "    callback = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = model.fit(x=train_dataset,\n",
        "                        epochs=num_epochs,\n",
        "                        verbose=0,\n",
        "                        callbacks=callback if num_epochs == 100 else None,\n",
        "                        validation_data=val_dataset\n",
        "                        )\n",
        "\n",
        "    val_loss, val_accuracy, val_precision, val_recall = \\\n",
        "        model.evaluate(val_dataset, verbose=0)\n",
        "\n",
        "    _plot_history(\n",
        "        history=history,\n",
        "        conv_filters=conv_filters,\n",
        "        conv_kernels=conv_kernels,\n",
        "        activation=activation,\n",
        "        pool=pool,\n",
        "        num_fc_layers=num_fc_layers\n",
        "    )\n",
        "\n",
        "    y_val = [np.argmax(res) for res in np.concatenate(\n",
        "        [y for x, y in val_dataset], axis=0)]\n",
        "    y_predict = [np.argmax(res) for res in model.predict(val_dataset)]\n",
        "    _plot_confusion_matrix(y_val, y_predict)\n",
        "\n",
        "    if give_model:\n",
        "        return val_loss, val_accuracy, val_precision, val_recall, model\n",
        "        \n",
        "    return val_loss, val_accuracy, val_precision, val_recall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OImtUefiki7c"
      },
      "source": [
        "### Task 4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErXDlCIjki7c",
        "outputId": "a04f808b-7987-4f75-8fa9-2247f12df84d"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(columns=[\n",
        "    'Convolution kernel_size',\n",
        "    'Convolution filters size',\n",
        "    'Pooling Layers',\n",
        "    'Activation',\n",
        "    'FC layer (after Flatten)',\n",
        "    'Dropout Rate',\n",
        "    'Validation Loss',\n",
        "    'Validation Accuracy',\n",
        "    'Validation Precision',\n",
        "    'Validation Recall'\n",
        "])\n",
        "\n",
        "convolution_kernels = [\n",
        "    [(3, 3), (3, 3), (3, 3)],\n",
        "    [(3, 3), (3, 3), (5, 5)],\n",
        "    [(3, 3), (5, 5), (5, 5)],\n",
        "    [(5, 5), (5, 5), (5, 5)],\n",
        "]\n",
        "convolution_filters = [16, 32, 64]\n",
        "activation = 'relu'\n",
        "dropout_rate = 0.1\n",
        "num_fc_layers = 0\n",
        "pool='max'\n",
        "\n",
        "for kernels in convolution_kernels:\n",
        "    val_loss, val_acc, val_precision, val_recall = train_model(\n",
        "        drop_rate=dropout_rate,\n",
        "        conv_kernels=kernels,\n",
        "        conv_filters=convolution_filters,\n",
        "        activation=activation,\n",
        "        pool=pool,\n",
        "        num_fc_layers=num_fc_layers,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    result.loc[len(result.index)] = [\n",
        "        kernels,\n",
        "        convolution_filters,\n",
        "        pool,\n",
        "        activation,\n",
        "        num_fc_layers,\n",
        "        dropout_rate,\n",
        "        val_loss,\n",
        "        val_acc,\n",
        "        val_precision, \n",
        "        val_recall \n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqM-FlZYki7c",
        "outputId": "edc2bcb0-5165-4198-b873-1e133725b803"
      },
      "outputs": [],
      "source": [
        "result['Validation f1'] = \\\n",
        "    2*result['Validation Precision']*result['Validation Recall']/(result['Validation Precision'] + result['Validation Recall'])\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkgLauKwki7d",
        "outputId": "b59a7df3-ef69-4755-e071-188878339470"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(\n",
        "    [str(res) for res in result['Convolution kernel_size']],\n",
        "    result['Validation Accuracy'])\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xlabel('Convolution Kernels')\n",
        "plt.title(\"Validation Accuracy for different Convolution Kernel\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVC9MPH7ki7d",
        "outputId": "2263e654-d877-4994-a770-b9cff43d2920"
      },
      "outputs": [],
      "source": [
        "best_conv_kernel = result.sort_values(\n",
        "    by=['Validation Accuracy', 'Validation Loss'],\n",
        "    ascending=[False, True]\n",
        ")['Convolution kernel_size'].iloc[0]\n",
        "\n",
        "best_conv_kernel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j85RXpLki7d"
      },
      "source": [
        "### Task 4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6OE7AnHki7d",
        "outputId": "ac705932-f8ce-4f5e-9932-5044289b6a01"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(columns=[\n",
        "    'Convolution kernel_size',\n",
        "    'Convolution filters size',\n",
        "    'Pooling Layers',\n",
        "    'Activation',\n",
        "    'FC layer (after Flatten)',\n",
        "    'Dropout Rate',\n",
        "    'Validation Loss',\n",
        "    'Validation Accuracy',\n",
        "    'Validation Precision',\n",
        "    'Validation Recall'\n",
        "])\n",
        "\n",
        "convolution_filters = [16, 32, 64]\n",
        "activation = 'relu'\n",
        "dropout_rate = 0.1\n",
        "num_fc_layers = [1, 2, 3]\n",
        "pool = 'max'\n",
        "\n",
        "for layers in num_fc_layers:\n",
        "    val_loss, val_acc, val_precision, val_recall = train_model(\n",
        "        drop_rate=dropout_rate,\n",
        "        conv_kernels=best_conv_kernel,\n",
        "        conv_filters=convolution_filters,\n",
        "        activation=activation,\n",
        "        pool=pool,\n",
        "        num_fc_layers=layers,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    result.loc[len(result.index)] = [\n",
        "        best_conv_kernel,\n",
        "        convolution_filters,\n",
        "        pool,\n",
        "        activation,\n",
        "        layers,\n",
        "        dropout_rate,\n",
        "        val_loss,\n",
        "        val_acc,\n",
        "        val_precision,\n",
        "        val_recall\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3-LqqEDki7d",
        "outputId": "1dd865d0-3f5e-4f44-c8d0-8e3fc769fc7d"
      },
      "outputs": [],
      "source": [
        "result['Validation f1'] = \\\n",
        "    2*result['Validation Precision']*result['Validation Recall']/(result['Validation Precision'] + result['Validation Recall'])\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pffqgpJjki7d",
        "outputId": "9cb98d3a-3454-4d04-ee4c-77795178007c"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(\n",
        "    result['FC layer (after Flatten)'],\n",
        "    result['Validation Accuracy'])\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xlabel('FC layer (after Flatten)')\n",
        "plt.title(\"Validation Accuracy for different FC layer (after Flatten)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7hjwMqdki7e",
        "outputId": "b571a407-cd26-4d4c-9beb-659295595c54"
      },
      "outputs": [],
      "source": [
        "best_num_fc_layers = result.sort_values(\n",
        "    by=['Validation Accuracy', 'Validation Loss'],\n",
        "    ascending=[False, True]\n",
        "    )['FC layer (after Flatten)'].iloc[0]\n",
        "\n",
        "best_num_fc_layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-ovi_moki7e"
      },
      "source": [
        "### Task 4.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks0CgMEoki7e",
        "outputId": "7d8851c6-3f36-4e31-da3b-c15ef05abba2"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(columns=[\n",
        "    'Convolution kernel_size',\n",
        "    'Convolution filters size',\n",
        "    'Pooling Layers',\n",
        "    'Activation',\n",
        "    'FC layer (after Flatten)',\n",
        "    'Dropout Rate',\n",
        "    'Validation Loss',\n",
        "    'Validation Accuracy',\n",
        "    'Validation Precision',\n",
        "    'Validation Recall'\n",
        "])\n",
        "\n",
        "convolution_filters = [16, 32, 64]\n",
        "activation = 'relu'\n",
        "dropout_rate = 0.1\n",
        "pools = ['max', 'avg']\n",
        "\n",
        "for pool in pools:\n",
        "    val_loss, val_acc, val_precision, val_recall = train_model(\n",
        "        drop_rate=dropout_rate,\n",
        "        conv_kernels=best_conv_kernel,\n",
        "        conv_filters=convolution_filters,\n",
        "        activation=activation,\n",
        "        pool=pool,\n",
        "        num_fc_layers=best_num_fc_layers,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    result.loc[len(result.index)] = [\n",
        "        best_conv_kernel,\n",
        "        convolution_filters,\n",
        "        pool,\n",
        "        activation,\n",
        "        best_num_fc_layers,\n",
        "        dropout_rate,\n",
        "        val_loss,\n",
        "        val_acc,\n",
        "        val_precision,\n",
        "        val_recall\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-odN2fJfki7e",
        "outputId": "91fe83cc-ebbf-43b0-de3a-292cacdc0e72"
      },
      "outputs": [],
      "source": [
        "result['Validation f1'] = \\\n",
        "    2*result['Validation Precision']*result['Validation Recall']/(result['Validation Precision'] + result['Validation Recall'])\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LkAY_wnki7e",
        "outputId": "c07068a5-9c1c-49f0-a008-1aa7b3cb5768"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(\n",
        "    result['Pooling Layers'],\n",
        "    result['Validation Accuracy'])\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xlabel('Pooling Layers')\n",
        "plt.title(\"Validation Accuracy for different Pooling Layers\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4VrJQy3ki7e",
        "outputId": "12b257c5-4457-4946-fb38-c7ab18be2e20"
      },
      "outputs": [],
      "source": [
        "best_pool = result.sort_values(\n",
        "    by=['Validation Accuracy', 'Validation Loss'],\n",
        "    ascending=[False, True]\n",
        "    )['Pooling Layers'].iloc[0]\n",
        "\n",
        "best_pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBnHBdRvki7e"
      },
      "source": [
        "### Task 4.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ0mdX_Rki7e",
        "outputId": "e0c5184e-f639-45ce-c9d1-a06ac1f64382"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(columns=[\n",
        "    'Convolution kernel_size',\n",
        "    'Convolution filters size',\n",
        "    'Pooling Layers',\n",
        "    'Activation',\n",
        "    'FC layer (after Flatten)',\n",
        "    'Dropout Rate',\n",
        "    'Validation Loss',\n",
        "    'Validation Accuracy',\n",
        "    'Validation Precision',\n",
        "    'Validation Recall'\n",
        "])\n",
        "\n",
        "convolution_filters = [16, 32, 64]\n",
        "activations = ['sigmoid', 'elu', 'relu', 'lrelu']\n",
        "dropout_rate = 0.1\n",
        "\n",
        "for activation in activations:\n",
        "    val_loss, val_acc, val_precision, val_recall = train_model(\n",
        "        drop_rate=dropout_rate,\n",
        "        conv_kernels=best_conv_kernel,\n",
        "        conv_filters=convolution_filters,\n",
        "        activation=activation,\n",
        "        pool=best_pool,\n",
        "        num_fc_layers=best_num_fc_layers,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    result.loc[len(result.index)] = [\n",
        "        best_conv_kernel,\n",
        "        convolution_filters,\n",
        "        best_pool,\n",
        "        activation,\n",
        "        best_num_fc_layers,\n",
        "        dropout_rate,\n",
        "        val_loss,\n",
        "        val_acc,\n",
        "        val_precision,\n",
        "        val_recall\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgQiwhcJki7e",
        "outputId": "616b92f0-cdff-45b4-bf0e-7fff358e698e"
      },
      "outputs": [],
      "source": [
        "result['Validation f1'] = \\\n",
        "    2*result['Validation Precision']*result['Validation Recall']/(result['Validation Precision'] + result['Validation Recall'])\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGfO05-Tki7e",
        "outputId": "7bb751fe-b832-4288-86e8-e6a9a4494450"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(\n",
        "    result['Activation'],\n",
        "    result['Validation Accuracy'])\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xlabel('Activation')\n",
        "plt.title(\"Validation Accuracy for different Activation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grfSXJ8Uki7f",
        "outputId": "70e2d5bf-faf7-46b7-e017-ec335034761d"
      },
      "outputs": [],
      "source": [
        "best_activation = result.sort_values(\n",
        "    by=['Validation Accuracy', 'Validation Loss'],\n",
        "    ascending=[False, True]\n",
        "    )['Activation'].iloc[0]\n",
        "\n",
        "best_activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eiLcncGki7f"
      },
      "source": [
        "### Task 4.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUP8ARiIki7f",
        "outputId": "b4437ae6-07a1-4f29-a521-4448a5380358"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(columns=[\n",
        "    'Convolution kernel_size',\n",
        "    'Convolution filters size',\n",
        "    'Pooling Layers',\n",
        "    'Activation',\n",
        "    'FC layer (after Flatten)',\n",
        "    'Dropout Rate',\n",
        "    'Batch Normalization Present',\n",
        "    'Validation Loss',\n",
        "    'Validation Accuracy',\n",
        "    'Validation Precision',\n",
        "    'Validation Recall'\n",
        "])\n",
        "\n",
        "convolution_filters = [16, 32, 64]\n",
        "dropout_rates = [0.1, 0.25, 0, 0.1]\n",
        "batch_norm_cases = [False, False, True, True]\n",
        "\n",
        "for dropout_rate, do_batch in zip(dropout_rates, batch_norm_cases):\n",
        "    val_loss, val_acc, val_precision, val_recall = train_model(\n",
        "        drop_rate=dropout_rate,\n",
        "        conv_kernels=best_conv_kernel,\n",
        "        conv_filters=convolution_filters,\n",
        "        activation=best_activation,\n",
        "        pool=best_pool,\n",
        "        add_batch_norm=do_batch,\n",
        "        num_fc_layers=best_num_fc_layers,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset\n",
        "    )\n",
        "\n",
        "    result.loc[len(result.index)] = [\n",
        "        best_conv_kernel,\n",
        "        convolution_filters,\n",
        "        best_pool,\n",
        "        best_activation,\n",
        "        best_num_fc_layers,\n",
        "        dropout_rate,\n",
        "        do_batch,\n",
        "        val_loss,\n",
        "        val_acc,\n",
        "        val_precision,\n",
        "        val_recall\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-8s1hTkki7f",
        "outputId": "9d085db8-ae16-4c76-b147-251d49afe5c4"
      },
      "outputs": [],
      "source": [
        "result['Validation f1'] = \\\n",
        "    2*result['Validation Precision']*result['Validation Recall']/(result['Validation Precision'] + result['Validation Recall'])\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoQRpAjbki7f",
        "outputId": "abe83067-04c9-439d-e243-9f27ad54d36b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(\n",
        "    [f'dropout {drop} batch {batch}' for drop,batch in zip(result['Dropout Rate'], result['Batch Normalization Present'])],\n",
        "    result['Validation Accuracy'])\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xlabel('Regularization')\n",
        "plt.title(\"Validation Accuracy for different Regularization\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uE8y_vDki7f",
        "outputId": "4bdbc942-41f6-4456-e978-b1e7265d83b0"
      },
      "outputs": [],
      "source": [
        "best_dropout = result.sort_values(\n",
        "    by=['Validation Accuracy', 'Validation Loss'],\n",
        "    ascending=[False, True]\n",
        "    )['Dropout Rate'].iloc[0]\n",
        "\n",
        "print(f\"best dropout rate: {best_dropout}\")\n",
        "\n",
        "best_do_batch = result.sort_values(\n",
        "    by=['Validation Accuracy', 'Validation Loss'],\n",
        "    ascending=[False, True]\n",
        "    )['Batch Normalization Present'].iloc[0]\n",
        "\n",
        "print(f\"Batch Normalization present for best case: {best_do_batch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVxVrj3bki7f"
      },
      "source": [
        "### Task 4.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32bmfOKwki7f",
        "outputId": "6b655e0a-b082-4e82-ea6c-0e55cc98b797"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "result = pd.DataFrame(columns=[\n",
        "    'Convolution kernel_size',\n",
        "    'Convolution filters size',\n",
        "    'Pooling Layers',\n",
        "    'Activation',\n",
        "    'FC layer (after Flatten)',\n",
        "    'Dropout Rate',\n",
        "    'Batch Normalization Present',\n",
        "    'Extra Conv Layers',\n",
        "    'Validation Loss',\n",
        "    'Validation Accuracy',\n",
        "    'Validation Precision',\n",
        "    'Validation Recall',\n",
        "    'Time Taken'\n",
        "])\n",
        "\n",
        "convolution_filters = [16, 32, 64]\n",
        "extra_conv_layers = [0, 1, 2, 3]\n",
        "\n",
        "for extra_conv_layer in extra_conv_layers:\n",
        "    # making a deep copy to avoid editing of original element\n",
        "    convolution_filters_copy = []\n",
        "    best_conv_kernel_copy = []\n",
        "\n",
        "    for filtr, kernel in zip(convolution_filters, best_conv_kernel):\n",
        "        convolution_filters_copy.append(filtr)\n",
        "        best_conv_kernel_copy.append(kernel)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    val_loss, val_acc, val_precision, val_recall = train_model(\n",
        "        drop_rate=best_dropout,\n",
        "        conv_kernels=best_conv_kernel_copy,\n",
        "        conv_filters=convolution_filters_copy,\n",
        "        activation=best_activation,\n",
        "        pool=best_pool,\n",
        "        add_batch_norm=best_do_batch,\n",
        "        num_fc_layers=best_num_fc_layers,\n",
        "        extra_conv_layers=extra_conv_layer,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "    )\n",
        "    end_time = time.time()\n",
        "\n",
        "    time_taken = end_time - start_time\n",
        "\n",
        "    result.loc[len(result.index)] = [\n",
        "        best_conv_kernel_copy,\n",
        "        convolution_filters_copy,\n",
        "        best_pool,\n",
        "        best_activation,\n",
        "        best_num_fc_layers,\n",
        "        best_dropout,\n",
        "        best_do_batch,\n",
        "        extra_conv_layer,\n",
        "        val_loss,\n",
        "        val_acc,\n",
        "        val_precision,\n",
        "        val_recall,\n",
        "        time_taken\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AgoTcG1ki7f",
        "outputId": "419add4e-14bd-4929-d99d-303d4c964d1d"
      },
      "outputs": [],
      "source": [
        "result['Validation f1'] = \\\n",
        "    2*result['Validation Precision']*result['Validation Recall']/(result['Validation Precision'] + result['Validation Recall'])\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzgy3venki7f",
        "outputId": "b702bc2e-600d-4909-916e-fc729e3035a7"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(\n",
        "    result['Extra Conv Layers'],\n",
        "    result['Validation Accuracy'])\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xlabel('Extra Conv Layers')\n",
        "plt.title(\"Validation Accuracy for different Extra Conv Layers\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPUqq2AFki7f",
        "outputId": "99480500-ec0b-4165-a05e-989b8fce79a6"
      },
      "outputs": [],
      "source": [
        "best_extra_conv_layer = result.sort_values(\n",
        "    by=['Validation Accuracy', 'Validation Loss'],\n",
        "    ascending=[False, True]\n",
        "    )['Extra Conv Layers'].iloc[0]\n",
        "\n",
        "best_extra_conv_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yzhAln2ki7f"
      },
      "source": [
        "### Task 4.7\n",
        "\n",
        "For the best set of parameters obtained here repeat the experimentation for color images. And visualize the test result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnxie2Q-ki7f",
        "outputId": "4cb0d579-1433-4147-895e-159046650b80"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(columns=[\n",
        "    'Convolution kernel_size',\n",
        "    'Convolution filters size',\n",
        "    'Pooling Layers',\n",
        "    'Activation',\n",
        "    'FC layer (after Flatten)',\n",
        "    'Dropout Rate',\n",
        "    'Batch Normalization Present',\n",
        "    'Extra Conv Layers',\n",
        "    'Color Mode',\n",
        "    'Validation Loss',\n",
        "    'Validation Accuracy',\n",
        "    'Validation Precision',\n",
        "    'Validation Recall',\n",
        "])\n",
        "\n",
        "convolution_filters = [16, 32, 64]\n",
        "\n",
        "val_loss, val_acc, val_precision, val_recall = train_model(\n",
        "    drop_rate=best_dropout,\n",
        "    conv_kernels=best_conv_kernel,\n",
        "    conv_filters=convolution_filters,\n",
        "    activation=best_activation,\n",
        "    pool=best_pool,\n",
        "    add_batch_norm=best_do_batch,\n",
        "    num_fc_layers=best_num_fc_layers,\n",
        "    extra_conv_layers=best_extra_conv_layer,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset\n",
        ")\n",
        "\n",
        "result.loc[len(result.index)] = [\n",
        "    best_conv_kernel,\n",
        "    convolution_filters,\n",
        "    best_pool,\n",
        "    best_activation,\n",
        "    best_num_fc_layers,\n",
        "    best_dropout,\n",
        "    best_do_batch,\n",
        "    best_extra_conv_layer,\n",
        "    'grayscale',\n",
        "    val_loss,\n",
        "    val_acc,\n",
        "    val_precision,\n",
        "    val_recall\n",
        "\n",
        "]\n",
        "\n",
        "convolution_filters = [16, 32, 64]\n",
        "\n",
        "color_train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=DATASET_DIRECTORY_PATH,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",  # output of model will be softmax categorical\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    image_size=(80, 80),\n",
        "    shuffle=True,\n",
        "    seed=seed,  # same seed for both dataset so that no overlap happens\n",
        "    validation_split=0.1,  # 90:10 split\n",
        "    subset='training',\n",
        "    crop_to_aspect_ratio=False,\n",
        ")\n",
        "\n",
        "color_val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=DATASET_DIRECTORY_PATH,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",  # output of model will be softmax categorical\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    image_size=(80, 80),\n",
        "    shuffle=True,\n",
        "    seed=seed,\n",
        "    validation_split=0.1,  # 90:10 split\n",
        "    subset='validation',\n",
        "    crop_to_aspect_ratio=False,\n",
        ")\n",
        "\n",
        "val_loss, val_acc, val_precision, val_recall = train_model(\n",
        "    drop_rate=best_dropout,\n",
        "    conv_kernels=best_conv_kernel,\n",
        "    conv_filters=convolution_filters,\n",
        "    activation=best_activation,\n",
        "    pool=best_pool,\n",
        "    add_batch_norm=best_do_batch,\n",
        "    num_fc_layers=best_num_fc_layers,\n",
        "    extra_conv_layers=best_extra_conv_layer,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset\n",
        ")\n",
        "\n",
        "result.loc[len(result.index)] = [\n",
        "    best_conv_kernel,\n",
        "    convolution_filters,\n",
        "    best_pool,\n",
        "    best_activation,\n",
        "    best_num_fc_layers,\n",
        "    best_dropout,\n",
        "    best_do_batch,\n",
        "    best_extra_conv_layer,\n",
        "    'rgb',\n",
        "    val_loss,\n",
        "    val_acc,\n",
        "    val_precision,\n",
        "    val_recall\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JTCRBx4ki7g",
        "outputId": "e30993f2-a388-4423-a133-be7eec46eb89"
      },
      "outputs": [],
      "source": [
        "result['Validation f1'] = \\\n",
        "    2*result['Validation Precision']*result['Validation Recall']/(result['Validation Precision'] + result['Validation Recall'])\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdF7R8fwki7g",
        "outputId": "895a05ce-f4e0-4532-a9d3-82601bc0c4a0"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "plt.bar(\n",
        "    result['Color Mode'],\n",
        "    result['Validation Accuracy'])\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.xlabel('Color Mode')\n",
        "plt.title(\"Validation Accuracy for different Color Mode\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3lU76Qjki7g"
      },
      "source": [
        "## Task 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNDFUuTCki7g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def create_dataset(X, Y, batch_size=32):\n",
        "  \"\"\" Create train and test TF dataset from X and Y\n",
        "    The prefetch overlays the preprocessing and model execution of a training step. \n",
        "    While the model is executing training step s, the input pipeline is reading the data for step s+1.\n",
        "    AUTOTUNE automatically tune the number for sample which are prefeteched automatically. \n",
        "    \n",
        "    Keyword arguments:\n",
        "    X -- numpy array\n",
        "    Y -- numpy array\n",
        "    batch_size -- integer\n",
        "  \"\"\"\n",
        "  AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "  \n",
        "  dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "  dataset = dataset.shuffle(buffer_size=1000, reshuffle_each_iteration=True)\n",
        "  dataset = dataset.batch(batch_size).prefetch(AUTOTUNE)\n",
        "  \n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK4eaREEki7g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tensorflow.image import resize\n",
        "\n",
        "mnist_data = tf.keras.datasets.mnist.load_data()\n",
        "(x_train, y_train), (x_test, y_test) = mnist_data\n",
        "\n",
        "#taking less data due to memory limitation\n",
        "x_train = x_train[0:10000]\n",
        "x_test = x_test[0:2000]\n",
        "y_train = y_train[0:10000]\n",
        "y_test = y_test[0:2000]\n",
        "\n",
        "#resizing to 80x80 and mapping 0-255 to 0-1\n",
        "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
        "x_train = x_train/255\n",
        "x_train = resize(x_train, (80,80))\n",
        "\n",
        "\n",
        "x_test = np.reshape(x_test, (-1, 28, 28, 1))\n",
        "x_test = x_test/255\n",
        "x_test = resize(x_test, (80,80))\n",
        "\n",
        "y_train = pd.get_dummies(y_train).to_numpy()\n",
        "y_test = pd.get_dummies(y_test).to_numpy()\n",
        "\n",
        "mnist_train_dataset=create_dataset(x_train, y_train)\n",
        "mnist_val_dataset=create_dataset(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVswEuulki7g",
        "outputId": "d30d49c0-78a9-459c-da81-d448424776ea"
      },
      "outputs": [],
      "source": [
        "convolution_filters = [16, 32, 64]\n",
        "num_class = 10\n",
        "\n",
        "val_loss, val_acc, val_precision, val_recall, model = train_model(\n",
        "    drop_rate=best_dropout,\n",
        "    conv_kernels=best_conv_kernel,\n",
        "    conv_filters=convolution_filters,\n",
        "    activation=best_activation,\n",
        "    pool=best_pool,\n",
        "    add_batch_norm=best_do_batch,\n",
        "    num_fc_layers=best_num_fc_layers,\n",
        "    extra_conv_layers=best_extra_conv_layer,\n",
        "    train_dataset=mnist_train_dataset,\n",
        "    val_dataset=mnist_val_dataset,\n",
        "    give_model=True\n",
        ")\n",
        "\n",
        "print(f\"val_loss= {val_loss}\\nval_acc={val_acc}\\nval_precision={val_precision}\\nval_recall={val_recall}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJGAhuKMki7g",
        "outputId": "e6071f04-9b92-4958-ecdb-991b9e7e7eb5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random_idx = random.sample(range(0, len(x_test)), 10)\n",
        "\n",
        "img_to_predict = np.array([x_test[idx] for idx in random_idx])\n",
        "\n",
        "categorical_predictions = model.predict(img_to_predict)\n",
        "\n",
        "for img, cat_pred in zip(img_to_predict, categorical_predictions):\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.show()\n",
        "    pred = np.argmax(cat_pred)\n",
        "    print(f\"predict = {pred}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7 (main, Oct 26 2022, 14:14:16) [Clang 14.0.0 (clang-1400.0.29.102)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5f90c9c72ce04906de37d4c67574a63377e4fd7d3965d8d2cd20954f66a8e417"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
